def _train_model_thread(self, model_type, model_name, model_params, training_params):
        """Train model in a background thread."""
        try:
            # Configure the model trainer with callbacks
            self.model_trainer.set_callbacks({
                'on_epoch_end': self._on_epoch_end,
                'on_training_start': self._on_training_start,
                'on_training_end': self._on_training_end,
                'on_batch_end': self._on_batch_end
            })
            
            # Train the model based on its type
            if model_type == "Pre-trained":
                self.model_trainer.train_transformer(
                    data_processor=self.data_processor,
                    model_name=model_name,
                    pretrained_model=model_params['model_name'],
                    finetune=model_params['finetune'],
                    batch_size=training_params['batch_size'],
                    lr=training_params['learning_rate'],
                    epochs=training_params['epochs'],
                    optimizer=training_params['optimizer']
                )
            
            elif model_type == "LSTM":
                self.model_trainer.train_lstm(
                    data_processor=self.data_processor,
                    model_name=model_name,
                    num_layers=model_params['lstm_layers'],
                    hidden_size=model_params['lstm_hidden_size'],
                    embedding_dim=model_params['lstm_embedding_dim'],
                    dropout=model_params['lstm_dropout'],
                    bidirectional=model_params['bidirectional'],
                    rnn_type=model_params['lstm_rnn_type'],
                    batch_size=training_params['batch_size'],
       
